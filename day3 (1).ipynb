{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain_community langchainhub chromadb langchain langgraph tavily-python langchain-text-splitters langchain_openai"
      ],
      "metadata": {
        "id": "QVu8_oZBWhXG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import TavilyClient\n",
        "tavily = TavilyClient(api_key='')\n",
        "\n",
        "response = tavily.search(query=\"Where does Messi play right now?\", max_results=3)\n",
        "context = [{\"url\": obj[\"url\"], \"content\": obj[\"content\"]} for obj in response['results']]\n",
        "\n",
        "# You can easily get search result context based on any max tokens straight into your RAG.\n",
        "# The response is a string of the context within the max_token limit.\n",
        "\n",
        "response_context = tavily.get_search_context(query=\"Where does Messi play right now?\", search_depth=\"advanced\", max_tokens=500)\n",
        "\n",
        "# You can also get a simple answer to a question including relevant sources all with a simple function call:\n",
        "# You can use it for baseline\n",
        "response_qna = tavily.qna_search(query=\"Where does Messi play right now?\")\n"
      ],
      "metadata": {
        "id": "T9Dv-wfsWhUr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "21e597f9"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "333cbcf4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"<your-api-key>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KrPdcdB7WE8B"
      },
      "outputs": [],
      "source": [
        "# üì¶ ÌïÑÏàò Ìå®ÌÇ§ÏßÄ ÏûÑÌè¨Ìä∏\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing_extensions import TypedDict\n",
        "from typing import List\n",
        "from pprint import pprint\n",
        "\n",
        "# üåê LLM ÏÑ§Ï†ï\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# üìò Î¨∏ÏÑú Î°úÎìú Î∞è Î≤†ÌÑ∞Ïä§ÌÜ†Ïñ¥ Íµ¨ÏÑ±\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# üîç Ï≤¥Ïù∏ Íµ¨ÏÑ±\n",
        "json_rule = (\n",
        "    \"You must respond with a valid JSON object only. \"\n",
        "    \"Use double quotes for keys and values. \"\n",
        "    \"Respond with one of: {{\\\"score\\\": \\\"yes\\\"}} or {{\\\"score\\\": \\\"no\\\"}}. \"\n",
        "    \"No explanation. No markdown. No extra text.\"\n",
        ")\n",
        "\n",
        "relevance_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are a grader. Given a question and a document, decide if the document helps answer the question. \"\n",
        "     \"Respond ONLY with a JSON like {{\\\"score\\\": \\\"yes\\\"}} or {{\\\"score\\\": \\\"no\\\"}}. \"\n",
        "     \"Be generous. If the document mentions key ideas or terms, it's relevant.\"),\n",
        "    (\"human\", \"question: {question}\\n\\ndocument: {document}\")\n",
        "])\n",
        "relevance_grader = relevance_prompt | llm | JsonOutputParser()\n",
        "\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are a hallucination detector. Determine whether the answer is meaningfully supported by the documents. \"\n",
        "     \"You can consider paraphrased or reworded sentences to be valid. \"\n",
        "     \"As long as the core definition, fact, or concept appears in the documents, consider it supported.\\n\\n\"\n",
        "     \"Output only one of: {{\\\"score\\\": \\\"yes\\\"}} or {{\\\"score\\\": \\\"no\\\"}}, formatted as JSON.\"),\n",
        "    (\"human\", \"documents: {documents}\\n\\nanswer: {generation}\")\n",
        "])\n",
        "hallucination_grader = hallucination_prompt | llm | JsonOutputParser()\n",
        "\n",
        "answer_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", f\"You are a usefulness evaluator. {json_rule}\"),\n",
        "    (\"human\", \"question: {question}\\n\\nanswer: {generation}\")\n",
        "])\n",
        "answer_grader = answer_prompt | llm | JsonOutputParser()\n",
        "\n",
        "generate_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the question using the provided context. Use max 3 sentences. Say 'I don't know' if unsure.\"),\n",
        "    (\"human\", \"question: {question}\\n\\ncontext: {context}\")\n",
        "])\n",
        "generate_chain = generate_prompt | llm | StrOutputParser()\n",
        "\n",
        "# üì¶ ÏÉÅÌÉú ÌÉÄÏûÖ Ï†ïÏùò\n",
        "class RAGState(TypedDict):\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[Document]\n",
        "    retry_count: int\n",
        "\n",
        "# üßê ÎÖ∏Îìú Ï†ïÏùò\n",
        "def docs_retrieval(state):\n",
        "    docs = retriever.invoke(state[\"question\"])\n",
        "    return {\"documents\": docs, \"question\": state[\"question\"]}\n",
        "\n",
        "def relevance_checker(state):\n",
        "    relevant_docs = []\n",
        "    for doc in state[\"documents\"]:\n",
        "        result = relevance_grader.invoke({\"question\": state[\"question\"], \"document\": doc.page_content})\n",
        "        print(f\"Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: {result['score']}\")\n",
        "        if result[\"score\"] == \"yes\":\n",
        "            relevant_docs.append(doc)\n",
        "    if relevant_docs:\n",
        "        return {\"documents\": relevant_docs, \"question\": state[\"question\"]}\n",
        "    else:\n",
        "        return {\n",
        "            \"documents\": [],\n",
        "            \"question\": state[\"question\"],\n",
        "            \"retry_count\": 0,\n",
        "            \"__next__\": \"search_tavily\"\n",
        "        }\n",
        "\n",
        "def search_tavily(state):\n",
        "    print(\"---SEARCH TAVILY---\")\n",
        "    query = state[\"question\"]\n",
        "    results = tavily.search(query=query, max_results=3)\n",
        "\n",
        "    # URL Ìè¨Ìï®Ìï¥ÏÑú Î¨∏ÏÑú ÏÉùÏÑ±\n",
        "    docs = []\n",
        "    for item in results[\"results\"]:\n",
        "        content = item.get(\"content\", \"\")\n",
        "        url = item.get(\"url\", \"\")\n",
        "        docs.append(Document(page_content=content, metadata={\"source\": url}))\n",
        "\n",
        "    return {\"question\": query, \"documents\": docs}\n",
        "\n",
        "def generate_answer(state):\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in state[\"documents\"]])\n",
        "    generation = generate_chain.invoke({\"context\": context, \"question\": state[\"question\"]})\n",
        "    return {**state, \"generation\": generation}\n",
        "\n",
        "def regenerate_answer(state):\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in state[\"documents\"]])\n",
        "    generation = generate_chain.invoke({\"context\": context, \"question\": state[\"question\"]})\n",
        "    return {**state, \"generation\": generation, \"retry_count\": 1}\n",
        "\n",
        "def hallucination_checker(state):\n",
        "    print(\"---HALLUCINATION CHECKER Ïã§ÌñâÎê®---\")\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in state[\"documents\"]])\n",
        "    hallucination = hallucination_grader.invoke({\"documents\": context, \"generation\": state[\"generation\"]})[\"score\"]\n",
        "    print(f\"üß™ hallucination ÌåêÎã® Í≤∞Í≥º: {hallucination}\")\n",
        "\n",
        "    if hallucination == \"no\":\n",
        "        if state.get(\"retry_count\", 0) < 1:\n",
        "            print(\"üîÅ hallucination Ïã§Ìå® ‚Üí Ïû¨ÏÉùÏÑ± ÏãúÎèÑ\")\n",
        "            return \"retry\"\n",
        "        else:\n",
        "            print(\"‚ùå hallucination 2Ìöå Ïã§Ìå® ‚Üí Ï¢ÖÎ£å\")\n",
        "            return \"fail\"\n",
        "\n",
        "    useful = answer_grader.invoke({\"question\": state[\"question\"], \"generation\": state[\"generation\"]})[\"score\"]\n",
        "    print(f\"üéØ Ïú†Ïö©ÏÑ± ÌåêÎã® Í≤∞Í≥º: {useful}\")\n",
        "    return \"success\" if useful == \"yes\" else \"fail\"\n",
        "\n",
        "# LangGraph Ï°∞Î¶Ω\n",
        "graph = StateGraph(RAGState)\n",
        "graph.set_entry_point(\"docs_retrieval\")\n",
        "graph.add_node(\"docs_retrieval\", docs_retrieval)\n",
        "graph.add_node(\"relevance_checker\", relevance_checker)\n",
        "graph.add_node(\"search_tavily\", search_tavily)\n",
        "graph.add_node(\"generate_answer\", generate_answer)\n",
        "graph.add_node(\"regenerate_answer\", regenerate_answer)\n",
        "\n",
        "graph.add_edge(\"docs_retrieval\", \"relevance_checker\")\n",
        "graph.add_conditional_edges(\"relevance_checker\", lambda s: s.get(\"__next__\", \"generate_answer\"), {\n",
        "    \"generate_answer\": \"generate_answer\",\n",
        "    \"search_tavily\": \"search_tavily\"\n",
        "})\n",
        "graph.add_edge(\"search_tavily\", \"generate_answer\")\n",
        "\n",
        "graph.add_conditional_edges(\"generate_answer\", hallucination_checker, {\n",
        "    \"success\": END,\n",
        "    \"fail\": END,\n",
        "    \"retry\": \"regenerate_answer\"\n",
        "})\n",
        "graph.add_conditional_edges(\"regenerate_answer\", hallucination_checker, {\n",
        "    \"success\": END,\n",
        "    \"fail\": END,\n",
        "    \"retry\": END\n",
        "})\n",
        "\n",
        "rag_app = graph.compile()\n",
        "\n",
        "# Ïã§Ìñâ Ìï®Ïàò\n",
        "def run_rag(question: str):\n",
        "    state = {\n",
        "        \"question\": question,\n",
        "        \"generation\": \"\",\n",
        "        \"documents\": [],\n",
        "        \"retry_count\": 0\n",
        "    }\n",
        "\n",
        "    final_generation = None\n",
        "    final_documents = []\n",
        "    steps_taken = []\n",
        "    hallucination_failed = False\n",
        "\n",
        "    print(f\"\\nüìå ÏßàÎ¨∏: {question}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for step in rag_app.stream(state):\n",
        "        for node_name, result in step.items():\n",
        "            steps_taken.append(node_name)\n",
        "            print(f\"üîÑ Step Ïã§ÌñâÎê®: {node_name}\")\n",
        "\n",
        "            if \"generation\" in result:\n",
        "                final_generation = result[\"generation\"]\n",
        "\n",
        "            if \"documents\" in result:\n",
        "                final_documents = result[\"documents\"]\n",
        "\n",
        "            if node_name in [\"regenerate_answer\"] and result.get(\"retry_count\", 0) == 1:\n",
        "                hallucination_failed = True\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"\\nüìç Ï†ÑÏ≤¥ Ïã§Ìñâ Í≤ΩÎ°ú: {' ‚Üí '.join(steps_taken)}\")\n",
        "    print(f\"\\nüìÑ Í¥ÄÎ†® Î¨∏ÏÑú Í∞úÏàò: {len(final_documents)}\")\n",
        "\n",
        "    if hallucination_failed:\n",
        "        print(\"\\n‚ùå ÏµúÏ¢Ö ÌåêÎã®: hallucinationÏúºÎ°ú Ïù∏Ìï¥ ÎãµÎ≥Ä ÏÉùÏÑ± Ïã§Ìå®\")\n",
        "    else:\n",
        "        print(\"\\nüß† ÏÉùÏÑ±Îêú ÎãµÎ≥Ä:\")\n",
        "        print(final_generation)\n",
        "\n",
        "        # üìå Ï∂úÏ≤ò Ï∂úÎ†•\n",
        "        print(\"\\nüìé Ï∂úÏ≤ò:\")\n",
        "        seen = set()\n",
        "        for doc in final_documents:\n",
        "            title = doc.metadata.get(\"title\", \"Untitled\")\n",
        "            source = doc.metadata.get(\"source\")\n",
        "            key = (title, source)\n",
        "            if key in seen:\n",
        "                continue\n",
        "            seen.add(key)\n",
        "            if source:\n",
        "                print(f\"- [{title}]({source})\")\n",
        "            else:\n",
        "                print(f\"- {title}\")\n",
        "\n",
        "    print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_rag(\"what is prompt?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "163GVinIXrZR",
        "outputId": "e88427a9-85d4-44f7-c375-39c59da4a772"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìå ÏßàÎ¨∏: what is prompt?\n",
            "==================================================\n",
            "üîÑ Step Ïã§ÌñâÎê®: docs_retrieval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: yes\n",
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: yes\n",
            "üîÑ Step Ïã§ÌñâÎê®: relevance_checker\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---HALLUCINATION CHECKER Ïã§ÌñâÎê®---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ hallucination ÌåêÎã® Í≤∞Í≥º: yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Ïú†Ïö©ÏÑ± ÌåêÎã® Í≤∞Í≥º: yes\n",
            "üîÑ Step Ïã§ÌñâÎê®: generate_answer\n",
            "==================================================\n",
            "\n",
            "üìç Ï†ÑÏ≤¥ Ïã§Ìñâ Í≤ΩÎ°ú: docs_retrieval ‚Üí relevance_checker ‚Üí generate_answer\n",
            "\n",
            "üìÑ Í¥ÄÎ†® Î¨∏ÏÑú Í∞úÏàò: 4\n",
            "\n",
            "üß† ÏÉùÏÑ±Îêú ÎãµÎ≥Ä:\n",
            "A prompt in the context of language models refers to the input or instructions given to the model to guide its behavior and generate desired outcomes. Prompt Engineering involves crafting these prompts to effectively communicate with the model without altering its internal parameters. It is an empirical process that requires experimentation and heuristics to achieve alignment and steerability of the model's responses.\n",
            "\n",
            "üìé Ï∂úÏ≤ò:\n",
            "- [Prompt Engineering | Lil'Log](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_rag(\"Where does Messi play right now?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruN-ufNVdrch",
        "outputId": "54c8ac58-f55b-4cce-e301-5fc75ff72075"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìå ÏßàÎ¨∏: Where does Messi play right now?\n",
            "==================================================\n",
            "üîÑ Step Ïã§ÌñâÎê®: docs_retrieval\n",
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: no\n",
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: no\n",
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: no\n",
            "üîÑ Step Ïã§ÌñâÎê®: relevance_checker\n",
            "---SEARCH TAVILY---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step Ïã§ÌñâÎê®: search_tavily\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---HALLUCINATION CHECKER Ïã§ÌñâÎê®---\n",
            "üß™ hallucination ÌåêÎã® Í≤∞Í≥º: yes\n",
            "üéØ Ïú†Ïö©ÏÑ± ÌåêÎã® Í≤∞Í≥º: yes\n",
            "üîÑ Step Ïã§ÌñâÎê®: generate_answer\n",
            "==================================================\n",
            "\n",
            "üìç Ï†ÑÏ≤¥ Ïã§Ìñâ Í≤ΩÎ°ú: docs_retrieval ‚Üí relevance_checker ‚Üí search_tavily ‚Üí generate_answer\n",
            "\n",
            "üìÑ Í¥ÄÎ†® Î¨∏ÏÑú Í∞úÏàò: 3\n",
            "\n",
            "üß† ÏÉùÏÑ±Îêú ÎãµÎ≥Ä:\n",
            "Lionel Messi currently plays for Inter Miami in Major League Soccer (MLS).\n",
            "\n",
            "üìé Ï∂úÏ≤ò:\n",
            "- [Untitled](https://www.usatoday.com/story/sports/soccer/2024/12/19/lionel-messi-2025-schedule-inter-miami-argentina-mls/77089729007/)\n",
            "- [Untitled](https://www.sportingnews.com/us/soccer/news/lionel-messi-playing-today-status-lineup-inter-miami-2025/b87bb697bffbfbd6b7de8a7a)\n",
            "- [Untitled](https://www.usatoday.com/story/sports/soccer/2025/03/28/inter-miami-vs-philadelphia-union-time-tv-will-messi-play/82704252007/)\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_rag(\"When did Prompt Engineering become an Olympic sport?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFQRCT_rfX_Z",
        "outputId": "86d5e22d-336b-4f91-cda4-431cb92d22c0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìå ÏßàÎ¨∏: When did Prompt Engineering become an Olympic sport?\n",
            "==================================================\n",
            "üîÑ Step Ïã§ÌñâÎê®: docs_retrieval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: no\n",
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: no\n",
            "Î¨∏ÏÑú relevance ÌåêÎã® Í≤∞Í≥º: no\n",
            "üîÑ Step Ïã§ÌñâÎê®: relevance_checker\n",
            "---SEARCH TAVILY---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Step Ïã§ÌñâÎê®: search_tavily\n",
            "---HALLUCINATION CHECKER Ïã§ÌñâÎê®---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ hallucination ÌåêÎã® Í≤∞Í≥º: no\n",
            "üîÅ hallucination Ïã§Ìå® ‚Üí Ïû¨ÏÉùÏÑ± ÏãúÎèÑ\n",
            "üîÑ Step Ïã§ÌñâÎê®: generate_answer\n",
            "---HALLUCINATION CHECKER Ïã§ÌñâÎê®---\n",
            "üß™ hallucination ÌåêÎã® Í≤∞Í≥º: no\n",
            "‚ùå hallucination 2Ìöå Ïã§Ìå® ‚Üí Ï¢ÖÎ£å\n",
            "üîÑ Step Ïã§ÌñâÎê®: regenerate_answer\n",
            "==================================================\n",
            "\n",
            "üìç Ï†ÑÏ≤¥ Ïã§Ìñâ Í≤ΩÎ°ú: docs_retrieval ‚Üí relevance_checker ‚Üí search_tavily ‚Üí generate_answer ‚Üí regenerate_answer\n",
            "\n",
            "üìÑ Í¥ÄÎ†® Î¨∏ÏÑú Í∞úÏàò: 3\n",
            "\n",
            "‚ùå ÏµúÏ¢Ö ÌåêÎã®: hallucinationÏúºÎ°ú Ïù∏Ìï¥ ÎãµÎ≥Ä ÏÉùÏÑ± Ïã§Ìå®\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}